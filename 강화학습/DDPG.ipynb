{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./강화학습.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DQN은 Discrete 하며 low-dimensional action space만 고려한다.\n",
    "- High-dimensional, continuous action space에서는 DQN보다는 model-free, off-policy actor-critic algorithm인 DDPG가 더 적합하다.\n",
    "  - (사실 FrozenLake에는 안적합한듯? 로봇 축구할 때 적합한 것 같아서 가져옴)  \n",
    "  \n",
    "  \n",
    "#### Discrete action space:\n",
    "- 행동 값이 중간 값이 없고 0 or 1을 가짐\n",
    "  - [상,하,좌,우] 일 때 [1,0,0,0] 과 같이 행동을 선택함\n",
    "- 중간 값이 없으며 다양한 행동을 표현하려는 경우 행동의 개수를 늘려야 함\n",
    "- Q-Learning과 같은 valued-based 알고리즘으로 학습\n",
    "\n",
    "#### continuous action space:\n",
    "- 실수 값을 가진다\n",
    "  - [x축,y축,회전]일 때 [-0.4,2.0,40] 과 같이 값을 실수 값으로 지정할 수 있음\n",
    "- 값의 크기 표현 가능\n",
    "- 행동 값의 범위가 무한\n",
    "- policy-based 알고리즘으로 학습\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medipixel.github.io/post/2020-05-28-adventure_of_rl_in_vessel_3/#ref_17\n",
    "https://www.slideshare.net/ssuser41d7e01/ddpg-deep-deterministic-policy-gradient-139832691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minji",
   "language": "python",
   "name": "minji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
