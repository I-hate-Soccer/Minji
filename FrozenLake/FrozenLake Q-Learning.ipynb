{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "https://3months.tistory.com/173  \n",
    "https://apincan.tistory.com/41?category=860374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "# Q-Table 만들기(0으로 초기화), (number of state, action space)=(16,4)\n",
    "Q=np.zeros([env.observation_space.n,env.action_space.n])\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 행동 01. random noise\n",
    "- 현재 state에서 가능한 action에 따른 Q값에 random noise를 주어서, 이 값이 최대가 되는 action을 선택함.\n",
    "- Q값에 random noise를 주었기 때문에 무조건 최대 Q를 따르지 않음.\n",
    "- random noise를 (i+1)로 나누기 때문에 어느 정도 반복 이후 Q값이 수렴 하므로, 탐험 보다는 이용(exploit) 위주"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.8 #큐값 업데이트시 사용\n",
    "dis=0.99 # discount factor, 감가율\n",
    "num_episodes=50000 #몇 번 시도? ---> 왜 epoch라고 안하고 에피소드라고 할까..?\n",
    "reward_list=[] # 에피소드마다 보상의 합 저장하는 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  range(num_episodes):\n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "    \n",
    "    # 1000번 반복마다 lr 감소시킴 -> 이거 넣으면 정확도가 떨어짐...왜지ㅠ 0.95 같이 크게 해도 10분의 1로 줄어들음\n",
    "    #if num_episodes%1000==0:\n",
    "        #learning_rate*=0.1\n",
    "    \n",
    "    while not done:\n",
    "        # action중에 가장 보상이 큰 것 고름. random noise 추가하여 탐험+탐색\n",
    "        action = np.argmax(Q[state,:] + np.random.randn(1,env.action_space.n)/(i+1))\n",
    "        \n",
    "        # 새로운 state, rewrd, done 여부 반환\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Q러닝 식\n",
    "        Q[state, action] = Q[state, action] \\\n",
    "                     + learning_rate*(reward+dis*np.max(Q[new_state,:]-Q[state,action]))\n",
    "        \n",
    "\n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "    \n",
    "    reward_list.append(rAll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 0.57894\n",
      "Final Q-Table Values\n",
      "[[6.55750437e-01 5.84728289e-03 1.78688841e-02 7.53679646e-03]\n",
      " [3.12386349e-03 1.65441672e-02 3.90478387e-03 6.83404262e-01]\n",
      " [7.92980021e-01 2.65587716e-06 2.30936069e-03 4.10700490e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [6.77021480e-01 1.48506603e-03 0.00000000e+00 3.85538110e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.17673148e-05 9.34190377e-07 5.63622482e-01 9.58428548e-06]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 5.78652946e-03 3.88821004e-03 6.97682803e-01]\n",
      " [0.00000000e+00 8.58323519e-01 0.00000000e+00 0.00000000e+00]\n",
      " [4.13851575e-01 0.00000000e+00 2.07533343e-04 1.37986493e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.86241554e-03 9.77730356e-01 7.76844535e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00758617e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Score over time: \"+str(sum(reward_list)/num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)\n",
    "plt.bar(range(len(reward_list)),reward_list,color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 행동 02. E-greedy\n",
    "- 어떠한 확률 값 e를 주어 e의 확률로 exploration\n",
    "> e=0.99 --> 99%확률로 탐험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.8 #큐값 업데이트시 사용\n",
    "dis=0.99 # discount factor, 감가율\n",
    "num_episodes=50000 #몇 번 시도? ---> 왜 epoch라고 안하고 에피소드라고 할까..?\n",
    "reward_list=[] # 에피소드마다 보상의 합 저장하는 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_episodes) : \n",
    "    state = env.reset()\n",
    "    rAll = 0\n",
    "    done = False\n",
    "\n",
    "    # 1000번 반복마다 lr 감소시킴\n",
    "    if num_episodes%1000==0:\n",
    "        learning_rate*=0.1\n",
    "        \n",
    "    # exploration의 확률 --> 학습이 진행될 수록 입실론 값 감소해야함\n",
    "    e = 1./((i / 100) + 1)\n",
    "    \n",
    "    while not done : \n",
    "        \n",
    "        # E-Greedy 알고리즘\n",
    "        if np.random.rand(1) > e : #랜덤으로 뽑은 숫자가 입실론보다 클 경우\n",
    "            action = np.argmax(Q[state, :]) #탐욕정책\n",
    "            \n",
    "        else : \n",
    "            action = env.action_space.sample() #랜덤하게 행동 결정\n",
    "        \n",
    "        # 새로운 state, rewrd, done 여부 반환\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Q러닝 식\n",
    "        Q[state, action] = Q[state, action] \\\n",
    "                     + learning_rate*(reward+dis*np.max(Q[new_state,:]-Q[state,action]))\n",
    "        \n",
    "        rAll += reward\n",
    "        state = new_state\n",
    "        \n",
    "    reward_list.append(rAll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate : 0.02362\n",
      "Final Q-Table Values\n",
      "[[4.15044730e-284 2.74225972e-237 5.24375923e-235 2.74225969e-235]\n",
      " [2.52038595e-235 3.14768101e-189 3.14768101e-196 3.46563141e-186]\n",
      " [3.97434470e-138 7.94868941e-149 3.97434470e-178 3.14768101e-218]\n",
      " [3.14768101e-205 3.14771248e-186 2.49296336e-239 3.14768101e-186]\n",
      " [4.15044969e-286 0.00000000e+000 4.14629665e-285 2.01588994e-284]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [5.01811200e-097 5.01811200e-096 5.01861381e-201 3.14768101e-191]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [6.33600000e-206 0.00000000e+000 6.33663360e-055 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 6.33600000e-149 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 8.00000000e-043 8.00000000e-017]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Success rate : \"+str(sum(reward_list) / num_episodes))\n",
    "print(\"Final Q-Table Values\")\n",
    "print(Q)\n",
    "\n",
    "plt.bar(range(len(reward_list)), reward_list, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 질문\n",
    "- 예상했던 결과는 학습 많이 할 수록 더 정확도 높아질 줄 알았는데 균등하게 보이는 것 같다..\n",
    "- random noise에서 learning rate를 1000번 당 0.1로 줄여줄 때 정확도가 아주 많이(50분의 1정도...?) 감소하는데 이유는??? 그리고 0.95정도로 작게 줄여도 약간(10분의 1) 줄어든다.\n",
    "- e-greedy에서는 learning rate를 안 줄일 때 0.01174, 0.95로 줄일 때 0.04208, 0.1로 줄일 때 0.02362 가 나왔다.... 그런데 이거 여러번 리셋하고 다시했는데 결과가 10배 이상 차이남 랜덤때문에 이렇게 많이 차이날 수 있나??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minji",
   "language": "python",
   "name": "minji"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
